{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_filename = 'data/train.csv'\n",
    "data = pd.read_csv(train_filename, sep='\\t')\n",
    "data = data.fillna('')\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "test_filename = 'data/test.csv'\n",
    "data_test = pd.read_csv(test_filename, sep='\\t')\n",
    "data_test = data_test.fillna('')\n",
    "data_test['date'] = pd.to_datetime(data_test['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(df):\n",
    "    df['Year'] = pd.DatetimeIndex(df['date']).year\n",
    "    df['Month'] = pd.DatetimeIndex(df['date']).month\n",
    "    df['Day'] = pd.DatetimeIndex(df['date']).day\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%file submissions/starting_kit/feature_extractor.py\n",
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler\n",
    "\n",
    "def clean_str(sentence, stem=True):\n",
    "    english_stopwords = set(\n",
    "        [stopword for stopword in stopwords.words('english')])\n",
    "    punctuation = set(string.punctuation)\n",
    "    punctuation.update([\"``\", \"`\", \"...\"])\n",
    "    if stem:\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        return list((filter(lambda x: x.lower() not in english_stopwords and\n",
    "                            x.lower() not in punctuation,\n",
    "                            [stemmer.stem(t.lower())\n",
    "                             for t in word_tokenize(sentence)\n",
    "                             if t.isalpha()])))\n",
    "\n",
    "    return list((filter(lambda x: x.lower() not in english_stopwords and\n",
    "                        x.lower() not in punctuation,\n",
    "                        [t.lower() for t in word_tokenize(sentence)\n",
    "                         if t.isalpha()])))\n",
    "\n",
    "def strip_accents_unicode(s):\n",
    "    try:\n",
    "        s = unicode(s, 'utf-8')\n",
    "    except NameError:  # unicode is a default on python 3\n",
    "        pass\n",
    "    s = unicodedata.normalize('NFD', s)\n",
    "    s = s.encode('ascii', 'ignore')\n",
    "    s = s.decode(\"utf-8\")\n",
    "    return str(s)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "class FeatureExtractor(TfidfVectorizer):\n",
    "    \"\"\"Convert a collection of raw documents to a matrix of TF-IDF features. \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__(\n",
    "            input='content', encoding='utf-8',\n",
    "            decode_error='strict', strip_accents=None, lowercase=True,\n",
    "            preprocessor=None, tokenizer=None, analyzer='word',\n",
    "            stop_words=None, token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    "            ngram_range=(1, 1), max_df=1.0, min_df=1,\n",
    "            max_features=None, vocabulary=None, binary=False,\n",
    "            dtype=np.int64, norm='l2', use_idf=True, smooth_idf=True,\n",
    "            sublinear_tf=False)\n",
    "        \n",
    "    def fit(self, X_df, y=None):\n",
    "        \"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_documents : iterable\n",
    "            An iterable which yields either str, unicode or file objects.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        self._feat = np.array([' '.join(\n",
    "            clean_str(strip_accents_unicode(dd)))\n",
    "            for dd in X_df.statement])\n",
    "        super(FeatureExtractor, self).fit(self._feat)\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X_df, y=None):\n",
    "        self.fit(X_df)\n",
    "        return self.transform(self.X_df)\n",
    "\n",
    "    def transform(self, X_df):\n",
    "        X = np.array([' '.join(clean_str(strip_accents_unicode(dd)))\n",
    "                      for dd in X_df.statement])\n",
    "        check_is_fitted(self, '_feat', 'The tfidf vector is not fitted')\n",
    "        X = super(FeatureExtractor, self).transform(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%file submissions/starting_kit/classifier.py\n",
    "# -*- coding: utf-8 -*-\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "class Classifier(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        #self.clf = RandomForestClassifier()\n",
    "        self.clf = MLPClassifier(hidden_layer_sizes=(300, 300, 300))\n",
    "        #self.clf = SVC()\n",
    "        #self.clf = LogisticRegression(C=1e5, solver='sag')\n",
    "        #self.clf = MultinomialNB()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.clf.fit(X.todense(), y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.clf.predict(X.todense())\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.clf.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = data.truth\n",
    "X_tr = f(data.drop(\"truth\", axis=1)).drop(\"date\", axis=1)\n",
    "texto_train = X_tr.iloc[:,:7] \n",
    "fecha_train = X_tr.iloc[:,7:] \n",
    "\n",
    "y_test = data_test.truth\n",
    "X_te = f(data_test.drop(\"truth\", axis=1)).drop(\"date\", axis=1)\n",
    "texto_test = X_te.iloc[:,:7] \n",
    "fecha_test = X_te.iloc[:,7:] \n",
    "\n",
    "df_texto = texto_train.append(texto_test, ignore_index=True)\n",
    "df_fecha = fecha_train.append(fecha_test, ignore_index=True)\n",
    "df_y = y_train.append(y_test, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "job = pd.get_dummies(pd.DataFrame(df_texto['job']).replace(to_replace='', value='Desconocido'))\n",
    "source = df_texto['source'].astype('category').cat.codes\n",
    "\"\"\"state = pd.get_dummies(pd.DataFrame(df_texto['state']).replace({'':'Desconocido',\n",
    "                                                                   'Washington, D.C.':'District of Columbia',\n",
    "                                                                   'Washington state':'Washington',\n",
    "                                                                   'Rhode island':'Rhode Island',\n",
    "                                                                   'Tennessee':'Tennesse',\n",
    "                                                                   'Virgina':'Virginia',\n",
    "                                                                  'ohio':'Ohio'}))\"\"\"\n",
    "state = df_texto['state'].replace({'':'Desconocido', 'Washington, D.C.':'District of Columbia',\n",
    "                                       'Washington state':'Washington','Rhode island':'Rhode Island',\n",
    "                                        'Tennessee':'Tennesse', 'Virgina':'Virginia',\n",
    "                                       'ohio':'Ohio'}).astype('category').cat.codes\n",
    "ptm = pd.DataFrame((df_texto['subjects'].str.strip('[]')).str.replace(\"'\",\"\"))\n",
    "hdp = []\n",
    "for i in range(len(ptm.values)):\n",
    "    ptm_uni = [np.char.strip(x.split(',')) for x in ptm.values[i]]\n",
    "    hdp.append(ptm_uni)\n",
    "subjects = pd.Series((v[0] for v in hdp)).str.join(sep='*').str.get_dummies(sep='*')\n",
    "df_texto = df_texto.drop(['subjects'],axis=1)\n",
    "df_texto = pd.concat([df_texto, subjects],axis=1)\n",
    "df = pd.DataFrame(df_texto[['edited_by', 'researched_by']], \n",
    "                  columns=['edited_by', 'researched_by'])\n",
    "df[\"edited_by\"] = df[\"edited_by\"].str.split(\", \")\n",
    "df[\"researched_by\"] = df[\"researched_by\"].str.split(\", \")\n",
    "left = df.join(pd.DataFrame(mlb.fit_transform(df.pop('edited_by')),\n",
    "                          columns=mlb.classes_,\n",
    "                          index=df.index)).drop(['','researched_by'], axis=1)\n",
    "right =  df.join(pd.DataFrame(mlb.fit_transform(df.pop('researched_by')),\n",
    "                          columns=mlb.classes_,\n",
    "                          index=df.index)).drop([''], axis=1)\n",
    "e = set(left)\n",
    "r = set(right)\n",
    "df=right.add(2*left[list(e.intersection(r))]).fillna(right)\n",
    "edited_researched = pd.concat([df, 2*left[list(e.difference(r))]], axis=1)\n",
    "cl = FeatureExtractor()\n",
    "cl.fit(pd.DataFrame(df_texto['statement']))\n",
    "statement = cl.transform(pd.DataFrame(df_texto['statement']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='full', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "pca_statement = PCA(svd_solver='full')\n",
    "pca_statement.fit(statement.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4745"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.cumsum(pca_statement.explained_variance_ratio_) > 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca_statement.n_components = 4745\n",
    "X_statement = pca_statement.fit_transform(statement.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = df_y.values\n",
    "#X = np.array(pd.concat([edited_researched,job,source,state,subjects,pd.DataFrame(statement.toarray())], axis=1))\n",
    "#sparcido = sparse.csr_matrix(np.array(pd.concat([edited_researched, job, source, state, subjects], axis=1)))\n",
    "#X = sp.sparse.hstack([sparcido, statement])\n",
    "X = np.array(pd.concat([edited_researched,job,source,state,subjects,pd.DataFrame(X_statement)], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10460, 5204)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1025"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(normalize(X))\n",
    "np.argmax(np.cumsum(pca.explained_variance_ratio_) > 0.9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca.n_components = 1025\n",
    "X_pca = pca.fit_transform(normalize(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test = np.split(X_pca, [data.shape[0]])\n",
    "y_train, y_test = np.split(y, [data.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement.toarray()[statement.toarray() < 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.417156693186\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = Classifier()\n",
    "clf.fit(sparse.csr_matrix(X_pca), y)\n",
    "pred = clf.predict(sparse.csr_matrix(X_test))\n",
    "\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7569, 7753)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ramp_test_submission --quick-test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
