{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import pip\n",
    "import sys\n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')\n",
    "#os.system(\"python -m nltk.downloader popular\")\n",
    "def install(package):\n",
    "    pip.main(['install', package])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named matplotlib.pyplot",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9973a15ba483>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#import pandas as pd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./submissions/starting_kit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named matplotlib.pyplot"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError:\n",
    "    print 'pandas is not installed, installing it now!'\n",
    "    install('pandas')\n",
    "#import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import issparse, csr_matrix, hstack\n",
    "sys.path.append('./submissions/starting_kit')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import SparsePCA\n",
    "from sklearn.preprocessing import normalize, MultiLabelBinarizer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = 'data/train.csv'\n",
    "train = pd.read_csv(train_filename, sep='\\t')\n",
    "test = pd.read_csv('data/test.csv', sep='\\t')\n",
    "frames = [train, test]\n",
    "data = pd.concat(frames, axis=0)\n",
    "data = data.fillna('')\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "#data['year'] = data.date.dt.year\n",
    "#data['month'] = data.date.dt.month\n",
    "#data['day'] = data.date.dt.day\n",
    "data.drop('date', axis=1, inplace=True)\n",
    "categorical_columns = ['state','job','source']\n",
    "replacements = {'state':{'Washington, D.C.':'District of Columbia', 'Tennessee':'Tennesse','Virgina':'Virginia','Washington state':'Washington', 'ohio':'Ohio', 'Rhode island': 'Rhode Island'},'job':{'None':''}}\n",
    "data = data.replace(to_replace=replacements)\n",
    "#print(len(np.unique(data.state)))\n",
    "for col in categorical_columns:\n",
    "    data[col] = data[col].astype('category')\n",
    "    data[col] = data[col].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "df = pd.DataFrame(data[[\"researched_by\",'edited_by']], columns=[\"researched_by\",'edited_by'])\n",
    "df[\"edited_by\"] = df[\"edited_by\"].str.split(\", \")\n",
    "df[\"researched_by\"] = df[\"researched_by\"].str.split(\", \")\n",
    "data[\"subjects\"] = data[\"subjects\"].str.replace(\"'\", '').str.strip(\"[]\").str.split(\", \")\n",
    "left = df.join(pd.DataFrame(mlb.fit_transform(df.pop('edited_by')),\n",
    "                          columns=mlb.classes_,\n",
    "                          index=df.index)).drop(['','researched_by'], axis=1)\n",
    "right = df.join(pd.DataFrame(mlb.fit_transform(df.pop('researched_by')),\n",
    "                          columns=mlb.classes_,\n",
    "                          index=df.index)).drop([''], axis=1)\n",
    "data =  data.join(pd.DataFrame(mlb.fit_transform(data.pop('subjects')),\n",
    "                          columns=mlb.classes_,\n",
    "                          index=data.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16242, 442)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = set(left)\n",
    "r = set(right)\n",
    "df=right.add(2*left[list(e.intersection(r))]).fillna(right)\n",
    "data = pd.concat([data.drop(['researched_by','edited_by'],axis=1), df, 2*left[list(e.difference(r))]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file submissions/starting_kit/feature_extractor.py\n",
    "# -*- coding: utf-8 -*-f\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import string\n",
    "import unicodeX_df\n",
    "\n",
    "import sys\n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.sparse import issparse, csr_matrix, hstack\n",
    "sys.path.append('./submissions/starting_kit')\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import SparsePCA\n",
    "from sklearn.preprocessing import normalize, MultiLabelBinarizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler\n",
    "\n",
    "\n",
    "\n",
    "def clean_str(sentence, stem=True):\n",
    "    english_stopwords = set(\n",
    "        [stopword for stopword in stopwords.words('english')])\n",
    "    punctuation = set(string.punctuation)\n",
    "    punctuation.update([\"``\", \"`\", \"...\"])\n",
    "    if stem:\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        return list((filter(lambda x: x.lower() not in english_stopwords and\n",
    "                            x.lower() not in punctuation,\n",
    "                            [stemmer.stem(t.lower())\n",
    "                             for t in word_tokenize(sentence)\n",
    "                             if t.isalpha()])))\n",
    "\n",
    "    return list((filter(lambda x: x.lower() not in english_stopwords and\n",
    "                        x.lower() not in punctuation,\n",
    "                        [t.lower() for t in word_tokenize(sentence)\n",
    "                         if t.isalpha()])))\n",
    "\n",
    "\n",
    "def strip_accents_unicode(s):\n",
    "    try:\n",
    "        s = unicode(s, 'utf-8')\n",
    "    except NameError:  # unicode is a default on python 3\n",
    "        pass\n",
    "    s = unicodeX_df.normalize('NFD', s)\n",
    "    s = s.encode('ascii', 'ignore')\n",
    "    s = s.decode(\"utf-8\")\n",
    "    return str(s)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "class FeatureExtractor(TfidfVectorizer):\n",
    "    \"\"\"Convert a collection of raw documents to a matrix of TF-IDF features. \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__(\n",
    "            input='content', encoding='utf-8',\n",
    "            decode_error='strict', strip_accents=None, lowercase=True,\n",
    "            preprocessor=None, tokenizer=None, analyzer='word',\n",
    "            stop_words=None, token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    "            ngram_range=(1, 1), max_df=1.0, min_df=1,\n",
    "            max_features=None, vocabulary=None, binary=False,\n",
    "            dtype=np.int64, norm='l2', use_idf=True, smooth_idf=True,\n",
    "            sublinear_tf=False)\n",
    "\n",
    "    def fit(self, X_df, y=None):\n",
    "        \"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_documents : iterable\n",
    "            An iterable which yields either str, unicode or file objects.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        X_df = X_df.fillna('')\n",
    "        X_df['date'] = pd.to_datetime(X_df['date'])\n",
    "        X_df.drop('date', axis=1, inplace=True)\n",
    "        categorical_columns = ['state','job','source']\n",
    "        replacements = {'state':{'Washington, D.C.':'District of Columbia', 'Tennessee':'Tennesse','Virgina':'Virginia','Washington state':'Washington', 'ohio':'Ohio', 'Rhode island': 'Rhode Island'},'job':{'None':''}}\n",
    "        X_df = X_df.replace(to_replace=replacements)\n",
    "        for col in categorical_columns:\n",
    "            X_df[col] = X_df[col].astype('category')\n",
    "            X_df[col] = X_df[col].cat.codes\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        df = pd.X_dfFrame(X_df[[\"researched_by\",'edited_by']], columns=[\"researched_by\",'edited_by'])\n",
    "        df[\"edited_by\"] = df[\"edited_by\"].str.split(\", \")\n",
    "        df[\"researched_by\"] = df[\"researched_by\"].str.split(\", \")\n",
    "        X_df[\"subjects\"] = X_df[\"subjects\"].str.replace(\"'\", '').str.strip(\"[]\").str.split(\", \")\n",
    "        left = df.join(pd.X_dfFrame(mlb.fit_transform(df.pop('edited_by')),\n",
    "                                  columns=mlb.classes_,\n",
    "                                  index=df.index)).drop(['','researched_by'], axis=1)\n",
    "        right = df.join(pd.X_dfFrame(mlb.fit_transform(df.pop('researched_by')),\n",
    "                                  columns=mlb.classes_,\n",
    "                                  index=df.index)).drop([''], axis=1)\n",
    "        X_df =  X_df.join(pd.X_dfFrame(mlb.fit_transform(X_df.pop('subjects')),\n",
    "                                  columns=mlb.classes_,\n",
    "                                  index=X_df.index))\n",
    "        e = set(left)\n",
    "        r = set(right)\n",
    "        df=right.add(2*left[list(e.intersection(r))]).fillna(right)\n",
    "        X_df = pd.concat([X_df.drop(['researched_by','edited_by'],axis=1), df, 2*left[list(e.difference(r))]], axis=1)\n",
    "        #y = X_df.pop('truth').values\n",
    "        #X = hstack((csr_matrix(X_df[X_df.columns[1:]].values), news_sparse))\n",
    "        self._feat = np.array([' '.join(\n",
    "            clean_str(strip_accents_unicode(dd)))\n",
    "            for dd in X_df.statement])\n",
    "        super(FeatureExtractor, self).fit(self._feat)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X_df, y=None):\n",
    "        self.fit(X_df)\n",
    "        return self.transform(self.X_df)\n",
    "\n",
    "    def transform(self, X_df):\n",
    "        X = np.array([' '.join(clean_str(strip_accents_unicode(dd)))\n",
    "                      for dd in X_df.statement])\n",
    "        check_is_fitted(self, '_feat', 'The tfidf vector is not fitted')\n",
    "        X = super(FeatureExtractor, self).transform(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_extractor import FeatureExtractor\n",
    "fe = FeatureExtractor()\n",
    "fe.fit(pd.DataFrame(data['statement']))\n",
    "news_sparse = fe.transform(pd.DataFrame(data['statement']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['statement'],axis=1)\n",
    "y = data.pop('truth').values\n",
    "X = hstack((csr_matrix(data[data.columns[1:]].values), news_sparse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = SparsePCA(n_components=200, n_jobs=-1)\n",
    "X_pca = pca.fit_transform(normalize(csr_matrix(X)).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = np.split(X_pca, [train.shape[0]])\n",
    "y_train, y_test = np.split(y, [train.shape[0]])\n",
    "#X_train, X_test, y_train, y_test = train_test_split(csr_matrix(X_pca), y, shuffle=False, stratify=None)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file submissions/starting_kit/classifier.py\n",
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "class Classifier(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        #self.clf = MLPClassifier()\n",
    "        clf = LinearSVC(penalty='l2', dual=False,class_weight='balanced')\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.clf.fit(X.todense(), y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.clf.predict(X.todense())\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.clf.predict_proba(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifier import Classifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#clf = LogisticRegression(penalty='l1', fit_intercept=True, class_weight='balanced', solver='saga', multi_class='multinomial', n_jobs=-1)\n",
    "#clf = MLPClassifier(activation='logistic', solver='lbfgs', hidden_layer_sizes=300)\n",
    "clf = LinearSVC(penalty='l2', dual=True,class_weight='balanced')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ramp_test_submission --quick-test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
